{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cedd47",
   "metadata": {},
   "source": [
    "# Local Manga AI\n",
    "\n",
    "This notebook launches a fully-local pipeline: **Qwen2.5 → SDXL → Page Composer** and can optionally expose a public HTTPS URL via **Cloudflare Tunnel**.\n",
    "\n",
    "Model folders:\n",
    "- `models/qwen2.5/`\n",
    "- `models/sdxl/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cafbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress Hugging Face Hub deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*resume_download.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*local_dir_use_symlinks.*\")\n",
    "\n",
    "# Runtime settings (run this BEFORE loading any torch/diffusers models)\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "# On Lightning AI: set MANGA_AI_HF_TOKEN as a Secret/Env Var (do not hardcode tokens in notebooks)\n",
    "# os.environ[\"MANGA_AI_HF_TOKEN\"] = \"...\"\n",
    "\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF=\", os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))\n",
    "print(\"HF token present:\", bool(os.environ.get(\"MANGA_AI_HF_TOKEN\")))\n",
    "print(\"HF deprecation warnings suppressed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f525514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect project root (folder that contains both ./scripts and ./models)\n",
    "CWD = Path.cwd().resolve()\n",
    "ROOT = CWD\n",
    "\n",
    "if not ((ROOT / \"scripts\").exists() and (ROOT / \"models\").exists()):\n",
    "    if (ROOT.parent / \"scripts\").exists() and (ROOT.parent / \"models\").exists():\n",
    "        ROOT = ROOT.parent.resolve()\n",
    "\n",
    "if not ((ROOT / \"scripts\").exists() and (ROOT / \"models\").exists()):\n",
    "    raise RuntimeError(\n",
    "        \"Could not locate project ROOT. Expected folders 'scripts' and 'models' in the working directory (or its parent). \"\n",
    "        f\"CWD={CWD}\"\n",
    "    )\n",
    "\n",
    "# Make sure imports like `from scripts...` work\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"Python:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from scripts.model_downloader import ensure_models_downloaded\n",
    "\n",
    "# Lightning AI tip: use a persistent path if your workspace provides one.\n",
    "# You can override with MANGA_AI_MODELS_DIR.\n",
    "MODELS_DIR = Path(os.environ.get(\"MANGA_AI_MODELS_DIR\", str(Path(ROOT) / \"models\"))).resolve()\n",
    "QWEN_DIR = MODELS_DIR / \"qwen2.5\"\n",
    "QWEN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Models dir:\", MODELS_DIR)\n",
    "print(\"Qwen dir:\", QWEN_DIR)\n",
    "\n",
    "try:\n",
    "    ensure_models_downloaded(\n",
    "        qwen_dir=QWEN_DIR,\n",
    "        sdxl_dir=MODELS_DIR / \"sdxl\",  # unused by downloader (kept for compatibility)\n",
    "        hf_token=os.environ.get(\"MANGA_AI_HF_TOKEN\"),\n",
    "    )\n",
    "    print(\"Qwen model is present (downloaded if needed).\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Qwen model download/setup failed. \"\n",
    "        \"If the model is gated for you, accept the license on HuggingFace and set MANGA_AI_HF_TOKEN in Lightning Secrets. \"\n",
    "        f\"Original error: {e}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks (doesn't load the full ML pipelines)\n",
    "assert (QWEN_DIR / \"config.json\").exists(), \"Qwen config.json not found\"\n",
    "\n",
    "print(\"Sanity checks passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3742eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Deprecated) SDXL sanity checks removed because Animagine/SDXL was removed from the Qwen-only workflow.\n",
    "# Keep this cell as a no-op so running top-to-bottom doesn't fail.\n",
    "print(\"Skipping SDXL sanity checks (Animagine removed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91ef0c",
   "metadata": {},
   "source": [
    "## Launch Web UI (Lightning AI)\n",
    "\n",
    "Running the next cell will:\n",
    "- Start Gradio inside the Lightning runtime\n",
    "- Lightning will expose it as a public URL automatically\n",
    "\n",
    "Notes:\n",
    "- The app binds to `0.0.0.0` and uses the `PORT` environment variable if present.\n",
    "- You do **not** need Cloudflare Tunnel on Lightning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6db7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "from scripts.storyboard import load_qwen_backend\n",
    "\n",
    "# Lightning AI: bind to 0.0.0.0 and use a predictable port\n",
    "HOST = \"0.0.0.0\"\n",
    "PORT = int(os.environ.get(\"PORT\", os.environ.get(\"LIGHTNING_PORT\", \"7860\")))\n",
    "\n",
    "_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "backend = load_qwen_backend(model_dir=QWEN_DIR, dtype=_dtype)\n",
    "\n",
    "\n",
    "def _build_chat_input(tokenizer: Any, messages: List[dict]) -> torch.Tensor:\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        return tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    joined = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "    return tokenizer(joined, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def _chat_generate(\n",
    "    history: List[Tuple[str, str]],\n",
    "    user_message: str,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float,\n",
    "    top_p: float,\n",
    "):\n",
    "    tokenizer = backend.tokenizer\n",
    "    model = backend.model\n",
    "\n",
    "    system = \"You are Qwen, a helpful assistant for manga story development.\"\n",
    "\n",
    "    messages: List[dict] = [{\"role\": \"system\", \"content\": system}]\n",
    "    for u, a in (history or []):\n",
    "        messages.append({\"role\": \"user\", \"content\": u})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": a})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message.strip()})\n",
    "\n",
    "    input_ids = _build_chat_input(tokenizer, messages).to(model.device)\n",
    "\n",
    "    do_sample = bool(temperature and float(temperature) > 0)\n",
    "    gen_kwargs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"max_new_tokens\": int(max_new_tokens),\n",
    "        \"do_sample\": do_sample,\n",
    "        \"temperature\": float(temperature) if do_sample else None,\n",
    "        \"top_p\": float(top_p) if do_sample else None,\n",
    "        \"repetition_penalty\": 1.05,\n",
    "        \"eos_token_id\": getattr(tokenizer, \"eos_token_id\", None),\n",
    "        \"pad_token_id\": getattr(tokenizer, \"pad_token_id\", getattr(tokenizer, \"eos_token_id\", None)),\n",
    "    }\n",
    "    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}\n",
    "\n",
    "    out = model.generate(**gen_kwargs)\n",
    "    decoded = tokenizer.decode(out[0][input_ids.shape[-1] :], skip_special_tokens=True).strip()\n",
    "\n",
    "    return (history or []) + [(user_message, decoded)], \"\"\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"Qwen Chat\") as demo:\n",
    "    gr.Markdown(\"# Qwen Chat\\nChat with your local Qwen model\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=520)\n",
    "    msg = gr.Textbox(label=\"Message\", lines=3)\n",
    "\n",
    "    with gr.Row():\n",
    "        send = gr.Button(\"Send\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    max_new_tokens = gr.Slider(128, 2048, value=512, step=32, label=\"Max new tokens\")\n",
    "    temperature = gr.Slider(0.0, 1.2, value=0.4, step=0.05, label=\"Temperature\")\n",
    "    top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label=\"Top-p\")\n",
    "\n",
    "    def _send(history, message, max_new_tokens, temperature, top_p):\n",
    "        if message is None or not str(message).strip():\n",
    "            return history, \"\"\n",
    "        return _chat_generate(history, str(message), max_new_tokens, temperature, top_p)\n",
    "\n",
    "    send.click(_send, inputs=[chatbot, msg, max_new_tokens, temperature, top_p], outputs=[chatbot, msg])\n",
    "    msg.submit(_send, inputs=[chatbot, msg, max_new_tokens, temperature, top_p], outputs=[chatbot, msg])\n",
    "    clear.click(lambda: [], outputs=[chatbot])\n",
    "\n",
    "print(f\"Starting Gradio on {HOST}:{PORT} (Lightning will expose this as a public URL)\")\n",
    "demo.launch(server_name=HOST, server_port=PORT, share=False, inbrowser=False, prevent_thread_lock=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
